{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is CCT, and why do we use it?\n",
    "\n",
    "**CCT** stands for **Compact Convolutional Transformer**. It is a powerful architecture used in **Computer Vision** problems because it combines the best features of **transformers** and **convolutional neural networks (CNNs)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features of CCT:\n",
    "- In **Vision Transformer (ViT)**, patches are required. But in **CCT**, the model starts by using **convolutional layers**.\n",
    "  - These layers extract important features from images, such as **edges** and **textures**.\n",
    "  - This helps in representing images better and ensures that the model can capture **local patterns** from the image.\n",
    "  \n",
    "- After the convolutional layers, **pooling** is required to reshape the images.\n",
    "  - This reshaped output works as a sequence for the **transformer model**.\n",
    "  \n",
    "- **Positional embedding** is usually needed for patches in **ViT**, but in **CCT**, positional embedding is optional.\n",
    "  - This is because the reshaped image already contains enough information for the transformer encoder.\n",
    "  \n",
    "---\n",
    "\n",
    "### Transformer Encoder in CCT:\n",
    "\n",
    "- The **Transformer Encoder** processes the sequence of image tokens (small pieces of the image) and learns how they relate to each other.\n",
    "- It captures both **local** and **global** patterns in the image.\n",
    "- The **self-attention** mechanism allows each token to look at every other token in the image and identify the important relationships.\n",
    "- This helps the model understand how different parts of the image, such as edges and textures, interact.\n",
    "  \n",
    "---\n",
    "\n",
    "### Sequence Pooling in CCT:\n",
    "\n",
    "- After the transformer encoder processes the tokens, **Sequence Pooling** gathers information from all tokens and creates a **single, condensed representation** of the entire image.\n",
    "- The pooled representation is an average (or sum) of all tokens, which is used for classification.\n",
    "- This step effectively summarizes the whole image into **one vector**, enabling the model to classify the image without needing to focus on individual patches anymore.\n",
    "\n",
    "---\n",
    "\n",
    "### MLP Head in CCT:\n",
    "\n",
    "- The **MLP Head** (Multilayer Perceptron) is the final part of the model.\n",
    "  - It takes the pooled representation from sequence pooling and predicts the class of the image (e.g., t-shirt, shoe, etc.).\n",
    "  - The MLP Head is a simple, fully connected neural network made up of one or more layers of neurons.\n",
    "  - These neurons process the pooled image representation and output the final prediction, like a 90% chance that the image is a \"t-shirt\".\n",
    "  \n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- **CCT** is particularly effective for **small datasets**, as it uses convolution to extract local features and combines it with transformers to understand **global relationships** within the image.\n",
    "- This gives it a strong ability to **recognize complex visual patterns**.\n",
    "- **ViT**, on the other hand, works well with **large datasets**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](model_sym.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we got a grip on CCT basics, we are going to implement CCT on the FashionMnist dataset.\n",
    "\n",
    "- At First we will import the necessary libraries and load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
