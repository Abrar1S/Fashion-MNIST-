{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is CCT, and why do we use it?\n",
    "\n",
    "**CCT** stands for **Compact Convolutional Transformer**. It is a powerful architecture used in **Computer Vision** problems because it combines the best features of **transformers** and **convolutional neural networks (CNNs)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features of CCT:\n",
    "- In **Vision Transformer (ViT)**, patches are required. But in **CCT**, the model starts by using **convolutional layers**.\n",
    "  - These layers extract important features from images, such as **edges** and **textures**.\n",
    "  - This helps in representing images better and ensures that the model can capture **local patterns** from the image.\n",
    "  \n",
    "- After the convolutional layers, **pooling** is required to reshape the images.\n",
    "  - This reshaped output works as a sequence for the **transformer model**.\n",
    "  \n",
    "- **Positional embedding** is usually needed for patches in **ViT**, but in **CCT**, positional embedding is optional.\n",
    "  - This is because the reshaped image already contains enough information for the transformer encoder.\n",
    "  \n",
    "---\n",
    "\n",
    "### Transformer Encoder in CCT:\n",
    "\n",
    "- The **Transformer Encoder** processes the sequence of image tokens (small pieces of the image) and learns how they relate to each other.\n",
    "- It captures both **local** and **global** patterns in the image.\n",
    "- The **self-attention** mechanism allows each token to look at every other token in the image and identify the important relationships.\n",
    "- This helps the model understand how different parts of the image, such as edges and textures, interact.\n",
    "  \n",
    "---\n",
    "\n",
    "### Sequence Pooling in CCT:\n",
    "\n",
    "- After the transformer encoder processes the tokens, **Sequence Pooling** gathers information from all tokens and creates a **single, condensed representation** of the entire image.\n",
    "- The pooled representation is an average (or sum) of all tokens, which is used for classification.\n",
    "- This step effectively summarizes the whole image into **one vector**, enabling the model to classify the image without needing to focus on individual patches anymore.\n",
    "\n",
    "---\n",
    "\n",
    "### MLP Head in CCT:\n",
    "\n",
    "- The **MLP Head** (Multilayer Perceptron) is the final part of the model.\n",
    "  - It takes the pooled representation from sequence pooling and predicts the class of the image (e.g., t-shirt, shoe, etc.).\n",
    "  - The MLP Head is a simple, fully connected neural network made up of one or more layers of neurons.\n",
    "  - These neurons process the pooled image representation and output the final prediction, like a 90% chance that the image is a \"t-shirt\".\n",
    "  \n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- **CCT** is particularly effective for **small datasets**, as it uses convolution to extract local features and combines it with transformers to understand **global relationships** within the image.\n",
    "- This gives it a strong ability to **recognize complex visual patterns**.\n",
    "- **ViT**, on the other hand, works well with **large datasets**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](model_sym.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we got a grip on CCT basics, we are going to implement CCT on the FashionMnist dataset.\n",
    "\n",
    "- At First we will import the necessary libraries and load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f201f47df45491a93b322a1b3d2eab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815a829b51944a2aa2e2dbc35b015dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17debc6af4041a78c63f48e2d590eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0855bd980155473c9e501d1e3c618cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the FashionMNIST dataset with normalization\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download the training dataset\n",
    "full_train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training (90%) and validation (10%)\n",
    "train_size = int(0.9 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets into DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset (which is kept aside for evaluation only)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
